# 4.1 파드를 안정적으로 유지하기

- 파드가 노드에 스케줄링되는 즉시, 해당 노드의 kubelet은 파드의 컨테이너를 실행하고 파드가 존재하는 한 컨테이너가 계속 실행되도록 할 것이다.
- 컨테이너의 주 프로세스에 충돌이 발생하면 kubelet이 컨테이너를 다시 시작한다.
    - 즉, 애플리케이션에 버그가 있어 충돌할 경우 쿠버네티스에서 자동으로 다시 시작한다.

그렇다면 충돌 없이도 작동이 중단되는 경우가 있을텐데, 그 경우(ex,메모리 누수)에는 어떻게 할까?

## 4.1.1 라이브니스 프로브(liveness probe) 소개

쿠버네티스는 라이브니스 프로브를 통해 컨테이너가 살아 있는지 확인할 수 있다. 파드의 스펙에 각 컨테이너의 라이브니스 프로브를 지정할 수 있다. 쿠버네티스는 주기적으로 프로브를 실행하고 프로브가 실패할 경우 컨테이너를 다시 시작한다.

### 컨테이너에 프로브를 실행하는 3가지 메커니즘

1. HTTP GET 프로브
    - 지정한 IP 주소, 포트, 경로에 http get 요청을 수행한다
    - 응답 코드를 반환하거나 응답하지 않으면 프로브가 실패한 것으로 간주된다.
2. TCP 소켓 프로브
    - 컨테이너의 지정된 포트에 TCP 연결을 시도한다.
    - 연결에 실패하면 컨테이너 재시작
3. Exec 프로브
    - 컨테이너 내의 임의의 명령을 실행하고 종료 상태 코드를 확인한다.
    - 상태 코드가 0이 아니면 실패

## 4.1.2 HTTP 기반 라이브니스 프로브 생성

```yaml
apiVersion: v1
kind: pod
metadata:
	name: kubia-liveness
spec:
	containers:
	- image: luksa/kubia-unhealthy
	  name: kubia
	  livenessProbe:
		httpGet:                   <- HTTP 기반 라이브니스 프로브
			path: /
			port: 8080
```

## 4.1.3 동작 중인 라이브니스 프로브 확인

```yaml
$ kget po kubia-liveness

NAME              READY   STATUS    RESTARTS   AGE
kubia-liveness              1/1     Running   3          21d
```

- RESTARTS 열에서 컨테이너가 몇번 다시 시작했는지 보여준다.

### 충돌된 컨테이너의 애플리케이션 로그 얻기

```yaml
$ kubectl logs mypod --previous
```

### 컨테이너가 다시 시작된 이유 확인하기

```yaml
$ kubectl describe po kubia-liveness

...
Containers:
  kubia:
    Container ID:
    Image:          luksa/kubia-unhealthy
    Image ID:
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  1
    Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
...

Events:
  Type    Reason          Age    From               Message
  ----    ------          ----   ----               -------
	Normal   Scheduled  2m17s             default-scheduler  Successfully assigned default/kubia-liveness to minikube
  Normal   Pulling    2m16s             kubelet            Pulling image "luksa/kubia-unhealthy"
  Normal   Pulled     70s               kubelet            Successfully pulled image "luksa/kubia-unhealthy" in 1m5.550142953s
  Normal   Created    70s               kubelet            Created container kubia
  Normal   Started    70s               kubelet            Started container kubia
  Warning  Unhealthy  7s (x2 over 17s)  kubelet            Liveness probe failed: HTTP probe failed with statuscode: 500
```

- EXIT CODE는 128 + x(프로세스에 전송된 시그널 번호) 이다.
- Events는 컨테이너의 이벤트에 대한 정보를 보여준다.
- 컨테이너가 종료되면 완전히 새로운 컨테이너가 생성된다.
    - 동일한 컨테이너X

## 4.1.4 라이브니스 프로브의 추가 속성 설정

```yaml
Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
```

- kubectl describe는 liveness probe에 관한 추가적인 정보도 표시된다.
- 지연(delay), 제한 시간(timeout), 기간(period) 등과 같은 추가 속성도 있다.
    - delay : 컨테이너가 시작된 후 얼마나 뒤에 프로브가 시작되는지
        - default는 0s이기 때문에, 자칫 애플리케이션이 요청을 받을 준비가 돼 있지 않아 실패하는 경우도 있으므로 주의해야한다.
    - timeout: timeout 대기 시간
    - period: probe 수행 간격(xx초마다 수행)
    - failure: 프로브가 연속 몇번 실패하면 컨테이너가 다시 시작하는지
    - 이런 추가적인 매개변수는 프로브를 정의할 때 지정할 수 있다.

## 4.1.5 효과적인 라이브니스 프로브 생성

운영 환경에서 실행 중인 파드는 반드시 liveness probe를 정의해야 한다.

### 라이브니스 프로브가 확인해야 할 사항

- 특정 url 경로(.../health)에 요청하도록하자.
    - 인증이 필요없게끔 주의하자

### 프로브를 가볍게 유지하기

- 많은 연산 리소스를 사용해서는 안 된다. 너무 많은 일을 하는 프로브는 컨테이너의 속도를 상당히 느려지게 만든다.
- 컨테이너에서 자바 애플리케이션을 실행하는 경우 라이브니스 정보를 얻기 위해 Exec 프로브로 전체 JVM을 새로 기동하는 대신, HTTP GET 라이브니스 프로브를 사용하자.

### 프로브에 재시도 루프를 구현하지 마라

- 실패 임계값을 1로 설정하더라도, 쿠버네티스는 실패를 한 번했다고 간주하기 전에 프로브를 여러 번 재시도한다. 따라서 프로브에 자체적인 재시도 루프를 구현하는 것은 헛수고다.

### 라이브니스 프로브 요약

- 이 작업은 파드를 호스팅하는 노드의 kubelet에서 수행한다. 마스터에서 실행 중인 쿠버네티스 컨트롤 플레인 구성 요소는 이 프로세스에 관여하지 않는다.
- 노드 자체에 충돌날 경우 모든 파드의 대체 파드를 생성해야하는 것은 컨트롤 플레인의 몫이다.
    - 직접 생성한 파드들은 kubelet에서만 관리된다.

# 4.2 레플리케이션컨트롤러 소개

- 레플리케이션컨트롤러는 쿠버네티스 리소스로서 파드가 항상 실행되도록 보장한다.
- 노드가 실패하면 레플리케이션컨트롤러가 관리하는 파드만 다시 생성된다.
- 일반적으로 레플리케이션컨트롤러는 파드와 여러 복제본을 작성하고 관리하기 위한 것이다.

## 4.2.1 레플리케이션컨트롤러의 동작

### 컨트롤러 조정 루프 소개

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8e05a476-e163-4fe8-86f9-4644d07ddcac/Untitled.png)

### 레플리케이션컨트롤러의 세 가지 요소 이해

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/373cda1f-0dad-444f-9688-6c75be801b49/Untitled.png)

- 레이블 셀렉터
    - 레플리케이션컨트롤러의 범위에 있는 파드를 결정
- 레플리카 수
    - 실행할 파드의 의도하는 수를 지정
- 파드 템플릿
    - 새로운 파드 레플리카를 만들 때 사용

### 컨트롤러의 레이블 셀렉터 또는 파드 템플릿 변경의 영향 이해

- 레이블 셀렉터와 파드 템플릿은 변경해도 기존 파드에는 영향을 미치지 않는다.
- 템플릿은 레플리케이션컨트롤러로 새 파드를 생성할 때만 영향을 미친다.

### 레플리케이션컨트롤러 사용 시 이점

- 기존 파드가 사라지면 새 파드를 시작해 파드가 항상 실행되도록 한다.
- 클러스터 노드에 장애가 발생하면 장애가 발생한 노드에서 실행 중인 모든 파드(레플리케이션컨트롤러의 제어하에 있는)에 관한 교체 복체본이 생성도니다.
- 수동 또는 자동으로 파드를 쉽게 수평으로 확장할 수 있게 한다.

## 4.2.2 레플리케이션컨트롤러 생성

```yaml
apiVersion: v1
kind: ReplicationController <- 레플리케이션컨트롤러의 매니페스트 정의
metadata:
	name: kubia <- 레플리케이션컨트롤러 이름
spec:
	replicas: 3  <- 의도하는 파드 인스턴스 수
	selector:  <- 파드 셀렉터로 레플리케이션 컨트롤러가 관리하는 파드 선택
		app: kubia

template: <- 새 파드에 사용되는 템플릿
	metadata:
		labels:
			app: kubia
	spec:
		containers:
		- name: kubia
			image: luksa/kubia
			ports:
			- containerPort: 8080
```

- 파일을 API 서버에 게시하면, 쿠버네티스는 레이블 셀렉터 app=kubia와 일치하는 파드 인스턴스가 세 개를 유지하도록 하는 kubia라는 이름의 새로운 레플리케이션컨트롤러를 생성한다.
- 템플릿의 파드 레이블은 레플리케이션컨트롤러의 레이블 셀렉터와 완전히 일치해야한다.
    - 그렇지 않으면 컨트롤러에서 새 파드를 무한정 생성할 수 있다.
- 셀렉터를 지정하지 않으면, 템플릿의 레이블로 자동 설정된다.

```yaml
$ kubectl create -f kubia-rc.yaml
replicationcontroller "kubia" created
```

## 4.2.3 레플리케이션컨트롤러 작동 확인

```yaml
# 레플리케이션 컨트롤러 정보 얻기
kubectl get rc

# 세부 정보 표시하기
kubectl describe rc kubia
```

### 컨트롤러가 새로운 파드를 생성한 원인 정확히 이해하기

- 통지(삭제됨)는 컨트롤러가 실제 파드 수를 확인하고, 적절한 조치를 취하도록 하는 트리거 역할을 한다.

### 노드 장애 대응

- 본 책에서 다음과 같은 예제 수행
    1. 3개의 노드에서 파드가 실행중
    2. 하나의 워커 노드 삭제
    3. 2개의 노드에서 삭제된 노드에서 실행중이던 파드를 생성

## 4.2.4 레플리케이션컨트롤러의 범위 안팎으로 파드 이동하기

- 파드의 레이블을 변경하면 레플리케이션컨트롤러의 범위에서 제거되거나 추가될 수 있다.
    - 한 레플리케이션컨트롤러에서 다른 레플리케이션컨트롤러로 이동할 수 있다.
- 파드는 metadata.ownerReferences 에서 레플리케이션컨트롤러를 참조한다.
    - 이 필드를 사용해 파드가 속한 rc를 쉽게 찾을 수 있다.
- 파드의 레이블 변경시 더 이상 아무도 이 파드를 관리하지 않는다.

### 컨트롤러에서 파드를 제거하는 실제 사례

특정 파드에 어떤 작업을 하려는 경우 사용(디버깅)

- 파드에 버그 발생할 경우, 파드를 rc 밖으로 빼내 원하는 방식으로 파드를 디버그하거나 문제를 재연하고 완료되면 해당 파드 삭제

### 레플리케이션컨트롤러의 레이블 셀렉터 변경

파드의 레이블을 변경하는 대신 rc의 레이블 셀렉터를 수정하면!?

- 답
    - 새로운 파드가 생성된다.

## 4.2.5 파드 템플릿 변경

파드 템플릿을 변경하는 것은 쿠키 커터를 교체하는 것과 같다.

- 즉, 새로운 파드에만 영향을 끼친다.
- 파드 전반에 영향을 주고싶으면 기존 파드를 삭제하면 된다.

## 4.2.6 수평 파드 스케일링

파드 수를 늘리거나 줄이는 것은 rc 리소스의 replicas 필드 값을 변경하기만 하면 된다.

### 레플리케이션컨트롤러 스케일 업/다운(확장/축소)하기

```yaml
# 이전 방식
$ kubectl scale rc kubia --replicas=10

# 레플리케이션컨트롤러의 정의를 편집해 스케일링하기
$ kubectl edit rc kubia
-> replicas: 3 # 해당값을 10으로 변경
# 해당 방법이 더 선언적이다.
# 저장시 바로 반영된다.
```

## 4.2.7 레플리케이션컨트롤러 삭제

- kubectl delete로 삭제할 경우 파드도 삭제된다.
    - kubectl delete rc kubia —cascade=false
        - cascade=false 옵션을 주면 파드는 유지된다.

# 4.3 레플리케이션컨트롤러 대신 레플리카셋 사용하기

초기에는 레플리케이션컨트롤러가 파드를 보게하고 노드 장애가 발생했을 때 재스케줄링하는 유일한 쿠버네티스 요소였다.

후에 레플리카셋이라는 유사한 리소스가 도입됐다. 현재는 레플리카셋만 사용중이다.

## 4.3.1 레플리카셋과 레플리케이션컨트롤러 비교

레플리카셋은 RC와 똑같이 동작하지만 좀 더 풍부한 표현식을 사용하는 파드 셀렉터를 갖고 있다.

- 레플리카셋은 특정 레이블이 없는 파드나 레이블의 값과 상관없이 매칭, RC는 특정 레이블이 있는 파드만 매칭.
- 레플리카셋은 다른 레이블(env=deval, env=production)을 동시 매칭 가능, RC는 불가능
- 레플리카셋은 값에 상관없이 레이블 키의 존재(ex: env=*)만으로 파드를 매칭 가능, RC는 불가능

## 4.3.2 레플리카셋 정의하기

```yaml
# 레플리카셋은 v1 API에서도 사용 가능, API그룹 apps버전 v1beta2에 속한다.
apiVersion: apps/v1beta2 
kind: ReplicaSet
metadata:
	name: kubia
spec:
	replicas: 3
	selector:
		matchLabels: # 레플리케이션컨트롤러와 유사한 간단한 matchLabels 셀렉터를 사용한다.
			app: kubia
	# 템플릿은 RC와 동일하다
	template:
		metadata:
			labels:
				app: kubia
		spec:
			containers:
			- name: kubia
				image: luksa/kubia
```

## 4.3.3. 레플리카셋 생성 및 검사

```yaml
$ k create ...
$ k get rs
$ k describe rs
```

## 4.3.4 레플리카셋의 더욱 표현적인 레이블 셀렉터 사용하기

레플리케이션컨트롤러에 비해 레플리카셋의 주요 개선 사항은 좀 더 표현적인 레이블 셀렉터다.

```yaml
# selector 예시

selector:
	matchExpressions:
		- key: app # 해당 셀렉터는 파드의 키가 "app"인 레이블을 포함해야 한다.
			operator: In
			values: # 레이블의 값은 kubia이다.
				- kubia
```

- In : 레이블의 값이 지정된 값 중 하나와 일치해야 한다.
- NotIn : 레이블의 값이 지정된 값과 일치하지 않아야 한다.
- Exists : 파드는 지정된 키를 가진 레이블이 포함돼야 하낟.
- DoesNotExist : 파드에 지정된 키를 가진 레이블이 포함돼 있지 않아야 한다.

여러 표현식을 지정하는 경우 셀렉터가 파드와 매칭되기 위해서는, 모든 표현식이 true여야 한다.

## 4.3.5 레플리카셋 정리

$ kubectl delete rs <name>

## 4.4 데몬셋을 사용해 각 노등서 정확히 한 개의 파드 실행하기

파드가 노드당 하나씩 필요한 경우가 있다.

- ex) 인프라 관련 파드(로그 수집기 등)

## 4.4.1 데몬셋으로 모든 노드에 파드 실행하기

모든 클러스터 노드마다 파드를 하나만 실행하려면 데몬셋 오브젝트를 생성해야한다. 그러면 파드가 클러스터 내에 무작위로 흩어져 배포되지 않는다.

- 데몬셋에 의해 생성되는 파드는 타깃 노드가 이미 지정돼 있고 쿠버네티스 스케줄러를 건너뛰는 것을 제외하면 레플리카셋과 매우 유사하다.
- 데몬셋에는 복제본이라는 개념이 없다.
- 노드가 다운되도 데몬셋은 다른 곳에 파드를 생성하지 않는다.
    - 그러나 새 노드가 클러스터에 추가되면 그곳에 파드를 추가 생성한다.
    - 기존 노드에서 파드가 삭제되는 경우도 마찬가지.

## 4.4.2 데몬셋을 사용해 특정 노드에서만 파드를 실행하기

- 데몬셋은 파드가 노드의 일부분에서만 실행되도록 지정하지 않으면 모든 노드에 파드를 배포한다.
    - 데몬셋 정의의 일부인 파드 템플릿에서 node-Selector 속성을 지정하면 된다.
    - 데몬셋은 파드가 노드에 배포되지 않도록 하는 노드(추후에 나온다고 함)에도 파드를 배포한다.

### 예제를 사용한 데몬셋 설명

- label이 disk=ssd인 노드에 ssd-monitor라는 데몬이 있다고 가정하자.
    
    ```yaml
    apiVersion: apps/v1beta2 # 현재 데몬셋은 v1
    kind: DaemonSet
    metadata:
    	name: ssd-monitor
    spec:
    	selector:
    		matchLabels:
    			app: ssd-monitor
    	template:
    		metadata:
    			labels:
    				app: ssd-monitor
    	spec:
    		nodeSelector: # 파드 템플릿은 disk=ssd 레이블이 있는 노드를 선택하는 노드 셀렉터를 갖는다.
    			disk: ssd
    		containers:
    		- name: main
    			image: luksa/ssd-monitor
    ```
    

### 데몬셋 생성

```yaml
$ kubectl create -f ssd-monitor-daemonset.yaml

$ k get ds

$ k get po
```

### 필요한 레이블을 노드에 추가하기

```yaml
$ k label node minikube disk=ssd
```

### 노드에서 레이블 제거하기

```yaml
$ kubectl label node minikube disk=hdd --overwrite
```

# 4.5 완료 가능한 단일 태스크를 수행하는 파드 실행

작업을 완료한 후에는 종료되는 테스크만 실행하려는 경우가 있을 것이다.

## 4.5.1 잡 리소스 소개

쿠버네티스는 잡(Job) 리소스로 이런 기능을 지원한다.

- 잡은 파드의 컨테이너 내부에서 실행 중인 프로세스가 성공적으로 완료되면 컨테이너를 다시 시작하지 않는 파드를 실행할 수 있다.
- 노드에 장애가 발생한 경우 해당 노드에 있던 잡이 관리하는 파드는 레플리카셋 파드와 같은 방식으로 다른 노드로 다시 스케줄링된다.
- 프로세스 장애의 경우 잡에서 컨테이너를 다시 시작할 것인지 설정 가능

## 4.5.2 잡 리소스 정의

```yaml
apiVersion: batch/v1 # 잡은 batch api 그룹, v1 속한다.
kind: Job
metadata:
	name: batch-job
spec: # 파드 셀렉터를 지정하지 않았다.(파드 템플릿의 레이블을 기반으로 만들어진다)
	template:
		metadata:
			labels:
				app: batch-job
	spec:
		restartPolicy: OnFailure # 잡은 기본 재시작 정책(Always)을 사용할 수 없다.
	containers:
	- name: main
		image: luka/batch-job
```

- 잡 파드는 무한정 실행하지 않으므로 기본 정책을 사용할 수 없다.
    - 따라서 restartPolicy를 OnFailure나 Never로 명시적으로 설정해야 한다.

## 4.5.3 파드를 실행한 잡 보기

```yaml
$ k get jobs

$ k get po

$ k get po -a

# 파드가 완료될 때 파드가 삭제되지 않는 이유는 해당 파드의 로그를 검사할 수 있게 하기 위해서다.
$ k logs batch-job-28qf4

$ k get job
```

## 4.5.5 잡에서 여러 파드 인스턴스 실행하기

잡은 두 개 이상의 파드 인스턴스를 생성해 병렬 또는 순차적으로 실행하도록 구성할 수 있다. 

- 이는 잡 스펙에 completions와 parallelism 속성을 설정해 수행한다.

### 순차적으로 잡 파드 실행하기

잡을 두 번 이상 실행해야 하는 경우 잡의 파드를 몇 번 실행할지를 completions에 설정한다.

```yaml
apiVersion: batch/v1 # 잡은 batch api 그룹, v1 속한다.
kind: Job
metadata:
	name: multi-completion-batch-job
spec: 
	completions: 5 # 이 잡은 다섯 개의 파드를 순차적으로 실행한다.
	template:

...

```

- 파드 중 하나가 실패하면 잡이 새 파드를 생성한다.

### 병렬로 잡 파드 실행하기

잡 파드를 하나씩 차례로 실행하는 대신 잡이 여러 파드를 병렬로 실행할 수도 있다.

```yaml
apiVersion: batch/v1 # 잡은 batch api 그룹, v1 속한다.
kind: Job
metadata:
	name: multi-completion-batch-job
spec: 
	completions: 5 # 이 잡은 다섯 개의 파드를 순차적으로 실행한다.
	parallelism: 2
	template:

...

```

- 잡은 파드를 두 개 생성하고 병렬로 실행한다.

### 잡 스케일링

잡이 실행되는 동안 잡읜 parallelism 속성을 변경할 수도 있다.

```yaml
$ kubectl scale job multi-completion-batch-job --replicas 3
```

- parallelism를 2에서 3으로 증가시켰다.

## 4.5.5 잡 파드가 완료되는 데 걸리는 시간 제한하기

- 파드 스펙에 activeDeadlineSeconds 속성을 설정해 파드의 실행 시간을 제한할 수 있다.
- 잡의 매니페스트에서 spec.backoffLimit 필드를 지정해 실패한 것으로 표시되기 전에 잡을 재시도할 수 있는 횟수를 설정할 수 있다.
    - default는 6

# 4.6 잡을 주기적으로 또는 한 번 실행되도록 스케줄링하기

쿠버네티스에서의 크론 작업은 크론잡(CronJob) 리소스를 만들어 구성한다.

## 4.6.1 크론잡 생성하기

```yaml
apiVersion: batch/v1 # 잡은 batch api 그룹, v1 속한다.
kind: CronJob
metadata:
	name: batch-job-every-fifteen-minutes
spec:
	schedule: "0, 15, 30, 45 * * * *" # 이 잡은 매일, 매시간 0,15,30,45 분에 실행한다.
	# 크론잡이 생성하는 잡 리소스의 템플릿
	jobTemplate:
		spec:
			template:
				metadata:
					labels:
						app: periodic-batch-job
				spec:
					restartPolicy: OnFailure
					containers:
					- name: main
						image: luka/batch-job
```

### 스케줄 설정하기

왼쪽에서 오른쪽으로 다섯 개의 항목을 갖고있다.

- 분 / 시 / 일 / 월 / 요일

### 잡 템플릿 설정하기

크론잡은 크론잡 스펙에 설정한 jobTemplate 특성에 따라 잡 리소스를 생성한다.

## 4.6.2 스케줄된 잡의 실행 방법 이해

잡 리소스는 대략 예정된 시간에 크론잡 리소스에서 생성된다.

- 잡이나 파드가 상대적으로 늦게 생성되고 실행될 수 있다. 예정된 시간을 너무 초과해 시작돼서는 안된다는 엄격한 요구 사항을 갖는 경우도 있다.
    - startingDeadlineSconds 필드를 지정해 데드라인을 설정 가능
        
        ```yaml
        spec:
        	schedule: "0, 15, 30, 45 * * * *" # 이 잡은 매일, 매시간 0,15,30,45 분에 실행한다.
        	startingDeadlineSeconds: 15 # 15초 안에 실행하지 못하면 실패로 표시된다.
        ```
