# 5. 서비스: 클라이언트가 파드를 검색하고 통신을 가능하게 함

* 특정 파드는 외부 요청 없이 독립적으로 작업을 수행할 수 있지만 대다수의 애플리케이션은 외부 요청에 응답을 위함
* 마이크로서비스의 경우 파드는 대개 클러스터 내부의 다른 파드나 클러스터 외부의 클라이언트에서 오는 HTTP 요청에 응답
* 파드가 다른 파드에게 제공하는 서비스를 사용하려면 다른 파드를 찾는 서비스가 필요
* 쿠버네티스에서는 다음과 같은 이유로 IP 주소나 호스트 이름을 지정하여 클라이언트 애플리케이션 구성을 할 수 없음
  * 파드는 일시적
  * 쿠버네티스는 노드에 파드를 스케줄링한 후 파드가 시작되기 직전에 IP 주소를 파드에 할당
  * 수평 스케일링은 여러 파드가 동일한 서비스를 제공할 수 있음을 의미하며 각 파드는 고유 IP가 존재  
    하지만 클라이언트는 서비스를 지원하는 파드의 수와 IP를 상관하지 않아야 함  
    즉, 모든 파드는 단일 IP 주소로 액세스가 가능해야 함
* 이러한 문제를 해결하기 위해 **서비스**라는 리소스를 제공

<br>

## 5.1 서비스 소개

* 서비스는 동일한 서비스를 제공하는 파드 그룹에 지속적인 단일 접점을 만들려고 할 때 생성하는 리소스
* 각 서비스는 서비스가 존재하는 동안 절대 바뀌지 않는 IP 주소와 포트가 존재
* 클라이언트는 이 IP 주소와 포트로 접속한 다음 해당 서비스를 지원하는 파드 중 하나에 연결

#### 예제를 통한 서비스 설명

> 프론트엔드 역할을 하는 파드는 여러 개, 백엔드 역할을 하는 파드는 한 개

* 다음과 같은 상황이 주어졌을 때 시스템을 기동하기 위해서는 두 가지 문제를 해결
  * 웹 서버가 하나든 여러 개는 상관없이 외부 클라이언트는 프론트엔드 파드에 연결
  * 프론트엔드 파드는 백엔드에 연결, 백엔드는 시간이 지남에 따라 클러스터 주위를 이동해 IP 주소가 바뀔 수 있음  
    따라서 주소가 바뀔 때마다 프론트엔드 파드를 재설정
* 이를 해결하기 위해 각 역할마다 서비스를 만들고 고정 IP를 만듦

### 5.1.1 서비스 생성

* 서비스 연결은 서비스 뒷단의 모든 파드로 로드밸런싱
* 레이블 셀렉터와 동일한 메커니즘으로 서비스에 속하는 파드들을 정의할 수 있음

#### kubectl expose로 서비스 생성

* 서비스를 생성하는 가장 쉬운 방법은 kubectl expose를 사용
* expose 명령어는 레플리케이션컨트롤러에서 사용된 것과 동일한 파드 셀렉터를 사용하여 서비스 리소스 생성, 모든 파드를 단일 IP 주소와 포트로 노출

#### YAML 디스크립터를 통한 서비스 생성

```yaml
apiVersion: v1
kind: Service
metadata:
 name: kubia
spec:
 ports:
 - port: 80
   targetPort: 8080
 selector:
  app: kubia
```

* kubia라는 서비스 정의
* 해당 서비스는 포트 80의 연결을 허용, app=kubia 레이블 셀렉터와 일치하는 파드의 포트 8080으로 라우팅

#### 새 서비스 검사하기

```
➜  kubenetes kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   4d23h
kubia        ClusterIP   10.104.151.137   <none>        80/TCP    11s
```

* 10.104.151.137에 서비스가 할당
* 클러스터 IP이므로 클러스터 내부에서만 액세스 할 수 있음

#### 클러스터 내에서 서비스 테스트

* 몇 가지 방법으로 클러스터 내부에서 서비스로 요청이 가능
  * 서비스 클러스터 IP로 요청을 보내고 응답을 로그로 남기는 파드를 만드는 것
  * 쿠버네티스 노드로 ssh 접속하고 curl 명령어 실행
  * kubectl exec 명령어로 기존 파드에서 curl 명령어 실행

#### 실행 중인 컨테이너에 원격으로 명령어 실행

* kubectl exec 명령어를 사용하면 기존 파드의 컨테이너 내에서 원격으로 임의의 명령어를 실행할 수 있음

  ```
  kubectl exec <pods-name> -- curl -s http://<service-cluster-ip>
  ```

#### 서비스의 세션 어피니티 구성

* 서비스 프록시가 각 연결을 임의의 파드로 선택해 연결을 전달 하기에 요청할 때마다 다른 파드가 선택

* 특정 클라이언트 모든 요청을 매번 같은 파드로 리다이렉션하려면 서비스의 세션 어피니티 속성을 기본값 None 대신 ClientIP로 설정

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
   name: kubia
  spec:
   sessionAffinity: ClientIP
   ...
  ```

* 쿠버네티스는 None과 ClientIP라는 두가시 유형의 서비스 어피니티만 지원

* 서비스는 TCP와 UDP 패킷을 처리하고 페이로드는 신경 X

* 쿠키의 경우 HTTP 프로토콜 구성이기에 서비스는 쿠키 자체를 모르며 세션 어피니티는 쿠키 기반으로 생성 불가

#### 동일한 서비스에서 여러 개의 포트 노출

* 서비스는 여러 포트를 지원 가능

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
   name: kubia
  spec:
   ports:
   - name: http
     port: 80
     targetPort: 8080
   - name: https
     port: 443
     targetPort: 8443
   selector:
    app: kubia
  ```

#### 이름이 지정된 포트 사용

* 각 파드의 포트에 이름을 지정하고 서비스 스펙에서 이름으로 참조 가능  
  포트 번호가 잘 알려진 경우가 아니더라도 서비스 스펙을 좀 더 명확하게 함

  ```yaml
  # 파드 설정
  kind: Pod
  spec:
   containers:
   - name: kubia
     ports:
     - name: http
       containerPort: 8080
     - name: https
       containerPort: 8443
       
  # 서비스 설정
  apiVersion: v1
  kind: Service
  spec:
   ports:
   - name: http
     port: 80
     targetPort: http
   - name: https
     port: 443
     targetPort: https
  ```

* 위와 같이 정의시 서비스 정의를 건들이지 않고 포트를 변경할 수 있음

### 5.1.2 서비스 검색

* 쿠버네티스는 클라이언트 파드가 서비스의 IP와 포트를 검색할 수 있는 방법 제공

#### 환경변수를 통한 서비스 검색

* 파드가 시작되면 쿠버네티스는 해당 시점에 존재하는 각 서비스를 가리키는 환경변수 세트를 초기화

* 클라이언트 파드를 생성하기 전에 서비스를 생성하면 해당 파드의 프로세스는 환경변수를 검사하여 서비스의 IP 주소와 포트를 얻을 수 있음

* 서비스에 대한 환경변수를 보려면 먼저 모든 파드를 삭제하고 레플리케이션컨트롤러에서 새로 파드를 만들어야 함

  ```
  ➜  kubenetes kubectl get pods
  NAME          READY   STATUS    RESTARTS   AGE
  kubia-lgwbc   1/1     Running   0          113s
  kubia-mb9j6   1/1     Running   0          113s
  kubia-svn5p   1/1     Running   0          113s
  ➜  kubenetes kubectl exec kubia-svn5p -- env
  PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  HOSTNAME=kubia-svn5p
  KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
  KUBERNETES_PORT_443_TCP_PROTO=tcp
  KUBIA_PORT_80_TCP_PORT=80
  KUBIA_SERVICE_HOST=10.104.151.137
  KUBIA_PORT=tcp://10.104.151.137:80

#### DNS를 통한 서비스 검색

* kube-dns 파드는 DNS 서버를 실행하며 클러스터에서 실행 중인 다른 모든 파드는 자동으로 이를 사용하도록 구성
* 파드에서 실행 중이 프로세스에서 수행된 모든 DNS 쿼리는 시스템에서 실행 중인 모든 서비스를 알고 있는 쿠버네티스 자체 DNS 서버로 처리(파드가 내부 DNS 서버를 사용할지 여부는 각 파드 스펙의 dnsPolicy 속성으로 구성 가능)
* 각 서비스는 내부 DNS 서버에서 DNS 항목을 가져오고 서비스 이름을 알고 있는 클라이언트 파드는 환경변수 대신 FQDN(정규화된 도메인 이름)으로 액세스 가능

#### FQDN을 통한 서비스 연결

```
<service-name>.<service-defined-namespace>.svc.cluster.local
```

* `svc.cluster.local` 은 모든 클러스터의 로컬 서비스 이름에 사용되는 클러스터의 도메인 접미사
* 연결할 파드가 동일한 네임스페이스에 있는 경우 접미사(`svc.cluster.local`)와 네임스페이스 생략

#### 파드의 컨테이너 내에세 셸 실행

```
➜  kubenetes kubectl exec -it kubia-svn5p bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@kubia-svn5p:/# curl http://kubia.default.svc.cluster.local
You've hit kubia-lgwbc
root@kubia-svn5p:/# curl http://kubia.default
You've hit kubia-lgwbc
root@kubia-svn5p:/# curl http://kubia
You've hit kubia-lgwbc
root@kubia-svn5p:/#
```

* 각 파드 컨테이너 내부의 DNS resolver가 구성돼 있기 때문에 네임스페이스와 `svc.cluster.local` 접미사를 생략 가능(컨테이서에서 /etc/resolv.conf 파일을 보면 이해)

  ```
  root@kubia-svn5p:/# cat /etc/resolv.conf
  nameserver 10.96.0.10
  search default.svc.cluster.local svc.cluster.local cluster.local
  options ndots:5

#### 서비스 IP에 핑을 할 수 없는 이유

```
root@kubia-svn5p:/# ping kubia
PING kubia.default.svc.cluster.local (10.104.151.137): 56 data bytes
^C--- kubia.default.svc.cluster.local ping statistics ---
68 packets transmitted, 0 packets received, 100% packet loss
```

* 서비스의 클러스터 IP가 가상 IP이므로 서비스 포트와 결합된 경우에만 의미가 있음

<br>

## 5.2 클러스터 외부에 있는 서비스 연결

* 서비스가 클러스터 내에 있는 파드로 연결을 전달하는 것이 아닌 외부 IP와 포트로 연결을 전달 할 수 있음(로드밸런싱, 서비스 검색 모두 활용 가능)

### 5.2.1 서비스 엔드포인트 소개

* 서비스는 파드에 직접 연결 X, 엔드포인트 리소스가 그 사이에 존재

  ```
  ➜  kubenetes kubectl describe svc kubia
  Name:              kubia
  Namespace:         default
  Labels:            <none>
  Annotations:       <none>
  Selector:          app=kubia
  Type:              ClusterIP
  IP Family Policy:  SingleStack
  IP Families:       IPv4
  IP:                10.104.151.137
  IPs:               10.104.151.137
  Port:              <unset>  80/TCP
  TargetPort:        8080/TCP
  Endpoints:         172.17.0.3:8080,172.17.0.4:8080,172.17.0.5:8080
  Session Affinity:  None
  Events:            <none>

* 엔트포인트 리소스는 서비스로 노출되는 파드의 IP 주소와 포트 목록

* 엔트포인트 리소스는 다른 리소스들과 유사하므로 kubectl get을 통해 기본 정보를 표시

  ```
  ➜  kubenetes kubectl get endpoints kubia
  NAME    ENDPOINTS                                         AGE
  kubia   172.17.0.3:8080,172.17.0.4:8080,172.17.0.5:8080   57m

* 파드 셀렉터는 서비스 스펙에 정의되어 있지만 들어오는 연결을 전달할 때 직접 사용 X
* 대신 셀렉터는 IP와 포트 목록을 작성하는 데 사용되며, 이는 엔드포인트 리소스에 저장
* 클라이언트가 서비스에 연결하면 서비스 프록시는 이들 중 하나의 IP와 포트 쌍을 선택하고 들어온 연결을 대상 파드의 수신 대기 서버로 전달

### 5.2.2 서비스 엔드포인트 수동 구성

* 서비스와 엔드포인트를 분리하면 수동으로 엔드포인트를 구성 가능

#### 셀렉터 없이 서비스 생성

```yaml
apiVersion: v1
kind: Service
metadata:
 name: external-service
spec:
 ports:
 - port: 80
```

#### 셀렉터가 없는 서비스에 관한 엔트포인트 리소스 생성

```yaml
apiVersion: v1
kind: Endpoints
metadata:
 name: external-service
subsets:
 - addresses:
   - ip: 11.11.11.11
   - ip: 22.22.22.22
   ports:
   - port: 80
```

* 엔드포인트 오브젝트는 서비스와 이름이 같아야 하고 서비스를 제공하는 대상 IP 주소와 포트 목록을 가져야 함
* 서비스와 엔드포인트 리소스가 모두 서버에 게시되면 파드 셀렉터가 있는 일반 서비스처럼 사용 가능
* 서비스가 만들어진 후 만들어진 컨테이너에는 서비스의 환경변수가 포함되며 IP:포트 쌍에 대한 모든 연결은 서비스 엔드포인트 간에 로드밸런싱

### 5.2.3 외부 서비스를 위한 별칭 생성

* 서비스의 엔드포인트를 수동으로 구성해 외부 서비스를 노출하는 대신 좀 더 간단한 방법으로 FQDN으로 외부 서비스를 참조 할 수 있음

#### ExternalName 서비스 생성

* 외부 서비스의 별칭으로 사용되는 서비스를 만들려면 유형 필드를 ExternalName으로 설정해 리소스를 만듦

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
   name: external-service
  spec:
   type: ExternalName
   externalName: someapi.somecompany.com
   ports:
   - port: 80

* 서비스가 생성되면 파드는 서비스의 FQDN을 사용하는 것이 아닌 external-service.default.svc.cluster.local 도메인 이름으로 외부 서비스에 연결
* ExternalName 서비스는 DNS 레벨에서만 구현, 서비스에 관한 간단한 CNAME DNS 레코드가 생성  
  따라서 서비스에 연결하는 클라이언트는 서비스 프록시를 완전히 무시하고 외부 서비스에 직접 연결
* 이러한 이유로 ExternalName 유형의 서비스는 ClusterIP를 얻지 못함

<br>

## 5.3 외부 클라이언트에 서비스 노출

* 특정 서비스를 외부에 노출해 외부 클라이언트가 액세스 할 수 있게 하는 데 몇 가지 방법 존재
  * 노드포트로 서비스 유형 설정
    * 노드포트 서비스의 경우, 각 클러스터 노드는 노드 자체에서 포트를 열고 해당 포트로 수신된 트래픽을 서비스로 전달
    * 해당 서비스는 내부 클러스터 IP와 포트로 액세스 할 뿐만 아니라 모든 노드의 전용 포트로도 액세스 가능
  * 서비스 유형을 노드포트 유형의 확장인 로드밸런서로 설정
    * 쿠버네티스가 실행 중인 클라우드 인프라에서 프로비저닝된 전용 로드밸런서로 서비스 액세스 가능
    * 로드밸런서는 트래픽을 모든 노드의 노드포트로 전달
    * 클라이언트는 로드밸런서의 IP로 서비스에 액세스
  * 단일 IP 주소로 여러 서비스를 노출하는 인그레스 리소스 만들기
    * HTTP 레벨에서 작동하므로 4계층 서비스보다 더 많은 기능을 제공

### 5.3.1 노드포트 서비스 사용

* 노드포트 서비스를 만들면 쿠버네티스는 모든 노드에 특정 포트를 할당하고 서비스를 구성하는 파드로 들어오는 연결을 전달
* 일반 서비스와 유사하지만 서비스의 내부 클러스터IP뿐만 아니라 모든 노드의 IP와 할당된 노드포트로 서비스에 액세스할 수 있음

#### 노드포트 서비스 생성

```yaml
apiVersion: v1
kind: Service
metadata:
 name: kubia-nodeport
spec:
 type: NodePort
 ports:
 - port: 80
   targetPort: 8080
   nodePort: 30123
 selector:
  app: kubi
```

* 노드 포트는 반드시 지정해야하는 것이 아님, 생략하면 알아서 임의의 포트 선택

#### 노드포트 서비스 확인

```
➜  kubenetes kubectl get svc kubia-nodeport
NAME             TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubia-nodeport   NodePort   10.101.125.88   <none>        80:30123/TCP   64s
```

* 책과 다름 EXTERNAL-IP 부분이 none으로 되어 있음

#### 외부 클라이언트가 노드포트 서비스에 액세스할 수 있도록 방화벽 규칙 변경

* GCP에서는 외부에서 연결을 하기 위해 방화벽 구성
* 생략

### 5.3.2 외부 로드밸런서로 서비스 노출

* 클라우드 공급자에서 실행되는 쿠버네티스 클러스터는 일반적으로 클라우드 인프라에서 로드밸런서를 자동으로 프로비저닝하는 기능을 제공
* 노드포트 대신 서비스 유형을 로드밸런서로 설정하기만 하면 됨
* 로드밸런서는 공개적으로 액세스 가능한 고유 IP 주소를 가지며 모든 연결을 서비스로 전달
* minikube에서는 기능 미제공

#### 로드밸런서 서비스 생성

```yaml
apiVersion: v1
kind: Service
metadata:
 name: kubia-loadbalancer
spec:
 type: LoadBalancer
 ports:
 - port: 80
   targetPort: 8080
 selector:
  app: kubia
```

#### 로드밸런서를 통한 서비스 연결

* 노드포트 서비스와는 달리 방화벽 설정을 할 필요가 없음
* 외부 클라이언트는 로드밸런서의 포트 80에 연결하고 노드에 암묵적으로 할당된 노드포트로 라우팅

### 5.3.3 외부 연결의 특성 이해

#### 불필요한 네트워크 홉의 이해와 예방

* 외부 클라이언트가 노드포트로 서비스에 접속할 경우 임의로 선택된 파드가 연결을 수신한 동일한 노드에서 실행 중일 수 있고 않을 수 도 있음
* 파드에 도달하려면 추가적인 네트워크 홉이 필요할 수 있으며 항상 바람직한 것으 아님
* 외부의 연결을 수신한 노드에서 실행 중인 파드로만 외부 트래픽을 전달하도록 서비스를 구성해 이 추가 홉을 방지 가능
* 서비스 스펙 섹션의 externalTrafficPolicy 필드를 설정
* 설정이 Local일 경우, 서비스 프록시는 로컬에 실행 중인 파드를 선택, 로컬 파드가 존재하지 않으면 연결이 중단  
  따라서 항상 서비스할 파드가 하나 이상 있는 노드에만 연결을 전달해야 함
* 일반적인 연결을 모든 파드에 균등하게 분산되지만 이 어노테이션을 사용할 경우 더 이상 적용 X

#### 클라이언트 IP가 보존되지 않음 인식

* 일반적으로 클러스터 내의 클라이언트가 서비스로 연결할 때 서비스의 파드는 클라이언트의 IP 주소를 얻을 수 있음

  그러나, 노드포트로 연결을 수신하면 패킷에서 소스 네트워크 주소 변환이 수행되므로 패킷의 소스 IP가 변경

* 파드는 실제 클라이언트 IP를 볼 수 없음, 이는 클라이언트의 IP를 알아야 하는 일부 애플리케이션에서 문제

<br>

## 5.4 인그레스 리소스로 서비스 외부 노출

#### 인그레스가 필요한 이유

* 로드밸런서 서비스는 자신의 공용 IP 주소를 가진 로드밸런서가 필요하지만, 인그레스는 한 IP 주소로 수십 개의 서비스에 접근이 가능하도록 지원
* 클라이언트가 HTTP 요청을 인그레스에 보낼 때, 요청한 호스트와 경로에 따라 요청을 전달할 서비스 결정
* 인그레스는 네트워크 스택의 애플리케이션 계층에서 작동하며 서비스가 할 수 없는 쿠키 기반 세션 어피니티 등과 같은 기능을 제공할 수 있음

#### 인그레스 컨트로럴라가 필요한 경우

* 쿠버네티스 환경마다 다른 컨트롤러 구현을 사용할 수 있지만, 일부는 기본 컨트롤러를 전혀 제공하지 않음

### 5.4.1 인그레스 리소스 생성

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
 name: kubia
spec:
 rules:
 - host: kubia.example.com
   http:
    paths:
    - path: /
      backend:
       serviceName: kubia-nodeport
       servicePort: 80
```

* Host kubia.example.com으로 요청되는 인그레스 컨트롤러에 수신된 모든 HTTP 요청을 80의 kubia-nodeport 서비스로 전송하는 규칙 적용

### 5.4.2 인그레스로 서비스 액세스

* http://kubia.example.com 서비스에 액세스하기위해서 도메인 이름이 인그레스 컨트롤러의 IP와 매핑

#### 인그레스의 IP 주소 얻기

* IP를 찾으려면 인그레스 목록을 확인

  ```
  ➜  kubenetes kubectl get ingresses
  NAME    CLASS    HOSTS               ADDRESS        PORTS   AGE
  kubia   <none>   kubia.example.com   192.168.64.2   80      32s

#### 인그레스 컨트롤러가 구성된 호스트의 IP를 인그레스 엔드포인트로 지정

* kubia.example.com을 해당 IP로 확인하도록 DNS 서버를 구성하거나 /etc/hosts에 다음을 추가

  ```
  <ingress-ip>	kubia.example.com
  ```

#### 인그레스로 파드 액세스

```
➜  kubenetes curl http://kubia.example.com
You've hit kubia-ghpvb
```

#### 인그레스 동작 방식

1. 클라이언트는 먼저 kubia.example.com의 DNS 조회를 수행
2. DNS 서버가 인그레스 컨트롤러의 IP를 반환
3. 클라이언트는 HTTP 요청을 인그레스 컨트롤러로 전송하고  host 헤더에서 kubia.example.com을 지정
4. 컨트롤러는 해당 헤더에서 클라이언트가 액세스하려는 서비스를 결정
5. 서비스와 관련된 엔트포인트 오브젝트로 파드 IP를 조회
6. 클라이언트 요청을 파드로 전달

### 5.4.3 하나의 인그레스로 여러 서비스 노출

* 인그레스 스펙을 자세히 보면 규칙과 경로가 모두 배열이므로 여러 항목을 가질 수 있음

#### 동일한 호스트의 다른 경로로 여러 서비스 매핑

```yaml
 - host: kubia.example.com
   http:
    paths:
    - path: /kubia
      backend:
       serviceName: kubia
       servicePort: 80
    - path: /bar
      backend:
       serviceName: bar
       servicePort: 80
```

#### 서로 다른 호스트로 서로 다른 서비스 매핑하기

```yaml
spec:
 rules:
 - host: foo.example.com
   http:
    paths:
    - path: /
      backend:
       serviceName: foo
       servicePort: 80
 - host: bar.example.com
   http:
    paths:
    - path: /
      backend:
       serviceName: bar
       servicePort: 80
```

### 5.4.4 TLS 트래픽을 처리하도록 인그레스 구성

#### 인그레스를 위한 TLS 인증서 생성

* 클라이언트가 인그레스 컨트롤러에 대한 TLS 연결을 시도하면 종료

* 클라이언트와 컨트롤러 간의 통신은 암호화되지만 컨트롤러와 백엔트 파드 간의 통신은 암호화 X

* 파드가 웹 서버를 실행하는 경우 HTTP 트래픽만 허용하고 인그레스 컨트롤러가 TLS와 관련된 모든 것을 처리하도록 할 수 있음

  그렇게 하기 위해서는 인그레스 컨트롤러가 인증서와 개인 키를 인그레스에 첨부

* 이 두 개는 시크릿이라는 쿠버네티스 리소스에 저장하며 인그레스 매니페스트에서 참조

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
 name: kubia
spec:
 tls:
 - hosts:
   - kubia.example.com
   secretName: tls-secret
 rules:
 - host: kubia.example.com
   http:
    paths:
    - path: /
      backend:
       serviceName: kubia-nodeport
       servicePort: 80
```

```
➜  kubenetes curl -k -v https://kubia.example.com/kubia
*   Trying 192.168.64.2...
* TCP_NODELAY set
* Connected to kubia.example.com (192.168.64.2) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/cert.pem
  CApath: none
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256
* ALPN, server accepted to use h2
* Server certificate:
*  subject: CN=kubia.example.com
*  start date: Sep 28 16:14:15 2021 GMT
*  expire date: Sep 23 16:14:15 2022 GMT
*  issuer: CN=kubia.example.com
*  SSL certificate verify result: self signed certificate (18), continuing anyway.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x7fb42380d800)
> GET /kubia HTTP/2
> Host: kubia.example.com
> User-Agent: curl/7.64.1
> Accept: */*
>
* Connection state changed (MAX_CONCURRENT_STREAMS == 128)!
< HTTP/2 200
< date: Tue, 28 Sep 2021 16:18:06 GMT
<
You've hit kubia-7tdw7
* Connection #0 to host kubia.example.com left intact
* Closing connection 0
```

<br>

## 5.5 파드가 연결을 수락할 준비가 됐을 때 신호 보내기

* 파드가 생성되는데 오랜 시간이 걸릴 때는 완전히 준비될 때까지 파드에 요청하지 않는 것이 좋음

### 5.5.1 레디니스 프로브 소개

* 레디니스 프로브는 주기적으로 호출되며 특정 파드가 클라이언트 요청을 수신할 수 있는지 결정
* 컨테이너의 레디니스 프로브가 성공을 반환하면 컨테이너가 요청을 수락할 준비가 됐다는 신호
* 준비가 돼었다는 신호는 컨테이너마다 다르므로 이를 확인하기 위한 상세 정보는 개발자의 몫

#### 레디니스 프로브 유형

* 세 가지 유형의 레디니스 프로브 존재
  * 프로세스를 실행하는 Exec 프로브는 컨테이너 상태를 프로세스의 종료 상태 코드로 결정
  * HTTP GET 프로브는  HTTP GET 요청을 컨테이너로 보내고 응답의 HTTP 상태 코드를 보고 컨테이너가 준비 됐는지 여부 결정
  * TCP 소켓 프로브는 컨테이너의 지정된 포트로 TCP 연결을 열고 이가 연결되면 준비된 것으로 간주

#### 레디니스 프로브의 동작

* 컨테이너가 시작될 때 쿠버네티스는 첫 번째 레디니스 점검을 수행하기 전에 구성 가능한 시간이 경과하도록 할 수 있음
* 그 다음 주기적으로 프로브를 호출하고 결과에 따라 작동
* 파드가 준비되지 않았다고 하면 서비스에서 제거
* 파드가 준비되면 서비스에 다시 추가
* 라이브니스 프로브와 달리 컨테이너가 준비 상태 점검에 실패하더라도 컨테이너가 종료되거나 다시 시작 X
* 레디니스 프로브가 실패하면 파드는 엔트포인트 오브젝트에서 제거

#### 레디니스 프로브가 중요한 이유

* 레디니스 프로브를 사용하면 클라이언트가 정상 상태인 파드하고만 통신하고 시스템에 문제가 있다는 것을 절대 알아차리지 않음

### 5.5.2 파드에 레디니스 프로브 추가

#### 파드 템플릿에 레디니스 프로브 추가

```yaml
   spec:
    containers:
    - name: kubia
      image: luksa/kubia
      readinessProbe:
       exec:
        command:
        - ls
        - /var/ready
```

* 새롭게 만든 파드는 내부에서 ls /var/ready 명령어를 주기적으로 수행, 현재는 해당 파일이 없으므로 레디니스 프로브 실패

#### 파드의 레디니스 상태 확인과 수정

```
➜  kubenetes kubectl get po
NAME          READY   STATUS    RESTARTS   AGE
kubia-5rb8t   0/1     Running   0          3m1s
kubia-lglpn   0/1     Running   0          2m27s
kubia-r4kpf   0/1     Running   0          111s
```

* 컨테이너 상태가 Running이지만 준비된 컨테이너가 없는 것을 볼 수 있음
* 파드 중 하나에 /var/ready 파일을 만들면 시작됨을 볼 수 있음

```
➜  kubenetes kubectl exec kubia-5rb8t -- touch /var/ready
➜  kubenetes kubectl get po
NAME          READY   STATUS    RESTARTS   AGE
kubia-5rb8t   1/1     Running   0          6m13s
kubia-lglpn   0/1     Running   0          5m39s
kubia-r4kpf   0/1     Running   0          5m3s
```

```
Readiness:exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1 #failure=3
```

* 레디니스 프로브는 기본적으로 10초마다 주기적으로 프로브가 실행

#### 하나의  READY 파드로 서비스를 호출

* 세개의 파드가 실행중이지만 준비는 하나의 파드만 되었으므로 준비 된 파드만 응답

### 5.5.3 실제 환경에서 레디니스 프로브가 수행해야 하는 기능

* 실제 환경에서는 레디니스 프로브는 애플리케이션이 클라이언트 요청을 수신할 수 있는지 여부에 따라 성공 또는 실패를 반환해야 함

#### 레디니스 프로브를 항상 정의하라

* 레디니스 프로브를 추가하지 않으면 파드가 시작하는 즉시 서비스 엔트포인드가 됨

  따라서 클라이언트는 "Connection refused" 유형의 에러를 볼 수 있음

#### 레디니스 프로브에 파드의 종료 코드를 포함하지 마라

* 파드가 종료할 때 실행되는 애플리케이션은 종료 신호를 받자마다 연결 수락을 중단

  따라서 종료 절차가 시작되는 즉시 레디니스 프로브가 실행하도록 만들어 모든 파드가 제거되어야 한다고 생각

  하지만, 필요없음 왜? 쿠버네티스는 파드를 삭제하자마자 모든 서비스에서 파드를 제거

<br>

## 5.6 헤드리스 서비스로 개별 파드 찾기

* 클라이언트가 모든 파드에 연결하려면 각 파드의 IP를 알아야 함
  * 한 가지 옵션은 클라이언트가 쿠버네티스  API 서버를 호출해 파드와 IP 주소 목록을 가져오도록 하는 것
  * 하지만 애플리케이션을 쿠버네티스와 무관하게 유지하려고 노력해야하기 때문에 API 서버는 바람직  X
* 쿠버네티스는 클라이언트가 DNS 조회로 파드 IP를 찾을 수 있도록 함
  * 일반적으로 서비스에 대한  DNS는 하나의 IP를 반환하나 쿠버네티스 서비스에 클러스터  IP가 필요하지 않으면  DNS 서버는 하나의 서비스  IP 대신 파드 IP들을 반환
* DNS 서버는 하나의  DNS A 레코드를 반환하는 대신 여러 개의 A 레코드를 반환
  * 각 레코드는 해당 시점에 서비스를 지원하는 개별 파드의  IP를 가리킴
  * 따라서 클라이언트는 간단한  DNS A 레코트 조회를 수행하고 서비스에 포함된 모든 파드 IP를 얻을 수 있음

### 5.6.1 헤드리스 서비스 생성

* 서비스 스펙의 clusterIP 필드를 None으로 설정하면 쿠버네티스는 클러스터 IP를 할당하지 않기에 서비스가 헤드리스 상태가 됨

```yaml
apiVersion: v1
kind: Service
metadata:
 name: kubia-headless
spec:
 clusterIP: None
 ports:
 - port: 80
   targetPort: 8080
 selector:
  app: kubia
```

### 5.6.2 DNS로 파드 찾기

#### YAML 매니페스트를 쓰지 않고 파드 실행

```
kubectl run dnsutils --image=tutum/dnsutils --command -- sleep infinity
```

* --generator 옵션은 존재하지 않음([참고](https://v1-17.docs.kubernetes.io/docs/reference/kubectl/conventions/#generators))

#### 헤드리스 서비스를 위해 반환된  DNS A 레코드

```
➜  kubenetes kubectl exec dnsutils -- nslookup kubia-headless
Server:		10.96.0.10
Address:	10.96.0.10#53

Name:	kubia-headless.default.svc.cluster.local
Address: 172.17.0.7
Name:	kubia-headless.default.svc.cluster.local
Address: 172.17.0.4
```

* DNS 서버는 `kubia-headless.default.svc.cluster.local` FQDN에 대해 서로 다른 두 개의 IP 반환
* 이는 바로 준비되었다고 보고된 두개의 파드

```
➜  kubenetes kubectl exec dnsutils -- nslookup kubia
Server:		10.96.0.10
Address:	10.96.0.10#53

Name:	kubia.default.svc.cluster.local
Address: 10.106.120.6
```

* 헤드리스가 아닌 서비스를  DNS가 반환하는 것은 클러스터  IP

* 헤드리스 서비스를 사용하더라도 클라이언트는 일반 서비스와 마찬가지로 파드에 연결 가능

  하지만 DNS 서버가 파드의 IP를 반환하기 때문에 클라이언트는 서비스 프록시 대신 파드에 직접 연결

### 5.6.3 모든 파드 검색 - 준비되지 않은 파드도 포함

* DNS 조회 메커니즘을 사용하여 준비되지 않은 파드도 찾을 수 있음

* 이를 하기 위해서는 다음과 같은 어노테이션을 추가

  ```yaml
  kind: Service
  metadata:
   annotations:
    service.alpha.kubernetes.io/tolerate-unread-endpoints: "true"
  ```

<br>

## 5.7 서비스 문제 해결

* 서비스로 파드에 액세스 할 수 없는 경우 다음과 같은 내용을 확인한 뒤에 다시 시작
  * 외부가 아닌 클러스터 내에서 서비스의 클러스터 IP에 연결되었는지 확인
  * 서비스 IP로 핑을 할 필요 없음
  * 레디니스 프로브를 정의했다면 성공했는지 확인
  * 파드가 서비스의 일부인지 확인하려면 kubectl get endpoints를 사용해 확인
  * FQDN이나 그 일부로 서비스에 액세스하려는데 작동하지 않는 경우,  FQDN 대신 클러스터 IP를 사용해 확인
  * 대상 포트가 아닌 서비스로 노출된 포트에 연결하고 있는지 확인
  * 파드  IP에 직접 연결해 파드가 올바른 포트에 연결돼 있는지 확인
  * 파드 IP로 애플리케이션에 액세스 할 수 없는 경우 애플리케이션이 로컬호스트에만 바인딩하고 있는지 확인
